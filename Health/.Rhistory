confusionMatrix(testData$Churn, predicted, threshold = optCutOff)
sensitivity(testData$Churn, predicted, threshold = optCutOff)
specificity(testData$Churn, predicted, threshold = optCutOff)
unlink('Desktop/BABI/Predictive Analytics/Logistic/Churn_Logit_Prediction_cache', recursive = TRUE)
require(randomForest)
fit=randomForest(Churn~., data=inputData)
library(caret)
varImp(fit)
varImpPlot(fit,type=2)
knitr::opts_chunk$set(echo = TRUE)
library(ROSE)
p <- (table(ChurnData$Churn)/sum(table(ChurnData$Churn)))[2]
library(ROSE)
p <- (table(ChurnData$Churn)/sum(table(ChurnData$Churn)))[2]
library(openxlsx)
library(DataExplorer)
library(openxlsx)
library(DataExplorer)
library(readxl)
setwd("/Users/balajivr/Downloads/")
ChurnData = read_xlsx("Dataset_Cellphone.xlsx", sheet = "Data")
#mydata1=read.csv("Datatransformation1.csv",header = TRUE)
attach(ChurnData)
obsdata=dim(ChurnData)
#### No of Observations in the Given Dataset = obsdata[1]
obsdata[2]
summary(ChurnData)
#### Structure of Given Dataset
cat("----------Using DataExplorer Package ----------")
plot_str(ChurnData)
#### Head values
head(ChurnData)
#### Tail values
tail(ChurnData)
cat("Are there any missing values, If so whats mean value : ", mean(is.na(ChurnData)), "\n")
plot_missing(ChurnData)
cat("Checking for imbalance in Dataset", "\n")
churnYes=length(which(ChurnData$Churn==0))
churnNo=length(which(ChurnData$Churn==1))
cat("Churning Customer Yes:", churnYes, "Churning Customer No:", churnNo, "\n")
cat("----------Using DataExplorer Package ----------", "\n")
plot_histogram(ChurnData)
cat("Printing the Correlation among all the variables in Dataset")
plot_correlation(ChurnData, type = 'all')
library(ROSE)
p <- (table(ChurnData$Churn)/sum(table(ChurnData$Churn)))[2]
ROSE.eval(Churn~., data=ChurnData, rpart, control.rose=list(p = p), seed=1)
library(rpart)
p <- (table(ChurnData$Churn)/sum(table(ChurnData$Churn)))[2]
ROSE.eval(Churn~., data=ChurnData, rpart, control.rose=list(p = p), seed=1)
# second classifier trained on ROSE balanced sample
# optional arguments to plot the roc.curve are passed through
# control.accuracy
ROSE.eval(Churn~., data=ChurnData, rpart,
control.rose=list(p = 0.5), control.accuracy = list(add.roc = TRUE,
col = 2), seed=1)
library(caTools)
#Split the Dataset into Train and Test dataset for Model creation
ChurnData$Split = sample.split(ChurnData, SplitRatio = 0.7)
TrainData = subset(ChurnData, ChurnData$Split==TRUE)
TestData = subset(ChurnData, ChurnData$Split==FALSE)
#Step1: Overall Validity of the Regression Model-Log Likelihood Test(Can we apply Regression or not)
logit=glm(Churn~.,data=TrainData, family=binomial)
library(lmtest)
lrtest(logit)
library(pscl)
pR2(logit)
#STEP 3: Whether Individual slopes or coefficients significance
summary(logit)
# #STEP 4 : Explanatary power of the odds
Odds=exp(coef(logit))
Probability=Odds/(1+Odds)
Probability
#STEP 5 : Classification / Confusion Matrix
Pred = predict(logit, type='response')
gg1 = floor(Pred+0.5)
table(Actual=TrainData$Churn,Prediction=gg1)
prediction = predict(logit, type='response',newdata = TestData)
table(Actual=TestData$Churn,Prediction=floor(prediction+0.5))
gg2=floor(prediction>(1/7))
table(gg2)
table(Actual=TestData$Churn,Prediction=gg2)
data <- data.frame(data)
wordcloud(corpus=rainbow(7), max.words = 50)
wordcloud(corpus,colors=rainbow(7), max.words = 50)
library(wordcloud)
wordcloud(corpus,colors=rainbow(7), max.words = 50)
#create corpus
corpus = Corpus(VectorSource(tweets$Tweet))
tweets=read.csv("/Users/balajivr/Desktop/BABI/WebSocialAnalytics/02_tweets.csv",stringsAsFactors =FALSE)
#Create Dependent Variable
tweets$Negative = as.factor(tweets$Avg<=-1)
table(tweets$Negative)
#install packages
install.packages("tm")
install.packages("tm")
library(tm)
library(SnowballC)
#create corpus
corpus = Corpus(VectorSource(tweets$Tweet))
library(wordcloud)
wordcloud(corpus,colors=rainbow(7), max.words = 50)
wordcloud(corpus,colors=rainbow(7), max.words = 150)
wordcloud(corpus,colors=rainbow(7), max.words = 50)
#Look at Stop words
stopwords("english")[1:10]
#Look at Stop words
stopwords("english")[1:100]
#Look at Stop words
stopwords("english")[1:10]
#Remove Punctuation
corpus = tm_map(corpus, removePunctuation)
#Look at Stop words
stopwords("english")[1:10]
#Remove stopwords and apple
#removeWords()
corpus =tm_map(corpus, removeWords, c("apple",stopwords("english")))
wordcloud(corpus,colors=rainbow(7), max.words = 50)
#Remove Punctuation
corpus = tm_map(corpus, removePunctuation)
#Look at Stop words
stopwords("english")[1:10]
#Look at Stop words
stopwords("english")[1:10]
#Remove stopwords and apple
#removeWords()
corpus =tm_map(corpus, removeWords, c("apple",stopwords("english")))
#Stem Document
corpus = tm_map(corpus,stemDocument)
wordcloud(corpus,colors=rainbow(7), max.words = 50)
frequencies = DocumentTermMatrix(corpus)
frequencies
table(frequencies)
dim(frequencies)
#convert to lower-case
corpus = tm_map(corpus, tolower)
tweets=read.csv("/Users/balajivr/Desktop/BABI/WebSocialAnalytics/02_tweets.csv",stringsAsFactors =FALSE)
#Create Dependent Variable
tweets$Negative = as.factor(tweets$Avg<=-1)
table(tweets$Negative)
#install packages
install.packages("tm")
library(tm)
install.packages("SnowballC")
library(SnowballC)
#create corpus
corpus = Corpus(VectorSource(tweets$Tweet))
#visualizing corpus for frequent terms
install.packages("wordcloud")
library(wordcloud)
wordcloud(corpus,colors=rainbow(7), max.words = 50)
#convert to lower-case
corpus = tm_map(corpus, tolower)
wordcloud(corpus,colors=rainbow(7), max.words = 50)
#Remove Punctuation
corpus = tm_map(corpus, removePunctuation)
#Look at Stop words
stopwords("english")[1:10]
#Remove stopwords and apple
#removeWords()
corpus =tm_map(corpus, removeWords, c("apple",stopwords("english")))
#Stem Document
corpus = tm_map(corpus,stemDocument)
#Creating Document Term Matrix(DTM)
frequencies = DocumentTermMatrix(corpus)
frequencies
install.packages("tm")
install.packages("SnowballC")
install.packages("wordcloud")
install.packages("wordcloud")
inspect(frequencies[1000:1005,505:515])
#Check for sparsity
findFreqTerms(frequencies,lowfreq = 20)
#Remove Sparse terms
sparse = removeSparseTerms(frequencies, 0.995)
#C
View(sparse)
#Convert to a data frame
tweetsSparse = as.data.frame(as.matrix(sparse))
View(tweetsSparse)
#Make all variable names R-friendly
colnames(tweetsSparse) = make.names(colnames(tweetsSparse))
#Add depdendent variable
tweetsSparse$Negative = tweets$Negative
library(rpart)
library(rpart)
library(rpart.plot)
tweetsModelCART=rpart(Negative~.,data=tweetsSparse,method="class")
prp(tweetsModelCART, extra=2)
library(randomForest)
tweetsRF=randomForest(Negative~.,data=tweetsSparse )
varImpPlot(tweetsRF)
tweetRFPred=predict(tweetsRF)
tweetRFPred
table(tweetRFPred)
table(tweetsSparse$Negative, tweetRFPred)
tweetRFPred=predict(tweetsRF, data=tweetsSparse)
tweetRFPred
table(tweetsSparse$Negative, tweetRFPred)
## TOPIC Analysis
sharktank=read.csv("/Users/balajivr/Desktop/BABI/WebSocialAnalytics/Shark Tank Companies.csv",stringsAsFactors =FALSE)
attach(shark)
## TOPIC Analysis
shark = read.csv("/Users/balajivr/Desktop/BABI/WebSocialAnalytics/Shark Tank Companies.csv",stringsAsFactors =FALSE)
attach(shark)
#Loading the packages for review
library(tm)
library(SnowballC)
library(wordcloud)
#create corpus
corpus = Corpus(VectorSource(shark$description))
#create corpus
corpusShark = Corpus(VectorSource(shark$description))
wordcloud(corpusShark,colors=rainbow(7), max.words = 50)
wordcloud(corpusShark,colors=rainbow(7), max.words = 100)
#convert to lower-case
corpusShark = tm_map(corpusShark, tolower)
wordcloud(corpusShark,colors=rainbow(7), max.words = 50)
wordcloud(corpusShark,colors=rainbow(7), max.words = 50)
#convert to lower-case
corpusShark = tm_map(corpusShark, tolower)
wordcloud(corpusShark,colors=rainbow(7), max.words = 50)
#Remove Punctuation
corpusShark = tm_map(corpusShark, removePunctuation)
#Look at Stop words
stopwords("english")[1:10]
#Remove stopwords and apple
#removeWords()
corpusShark =tm_map(corpusShark, removeWords, stopwords("english"))
#Stem Document
corpusShark = tm_map(corpusShark,stemDocument)
#Creating Document Term Matrix(DTM)
frequencies = DocumentTermMatrix(corpusShark)
frequencies
#Remove stopwords and apple
#removeWords()
corpusShark =tm_map(corpusShark, removeWords, c(stopwords("english")))
## TOPIC Analysis
shark = read.csv("/Users/balajivr/Desktop/BABI/WebSocialAnalytics/Shark Tank Companies.csv",stringsAsFactors =FALSE)
attach(shark)
table(shark$deal)
#Loading the packages for review
library(tm)
library(SnowballC)
library(wordcloud)
#create corpus
corpusShark = Corpus(VectorSource(shark$description))
wordcloud(corpusShark,colors=rainbow(7), max.words = 50)
#convert to lower-case
corpusShark = tm_map(corpusShark, tolower)
wordcloud(corpusShark,colors=rainbow(7), max.words = 50)
#Remove Punctuation
corpusShark = tm_map(corpusShark, removePunctuation)
#Look at Stop words
stopwords("english")[1:10]
#Remove stopwords and apple
#removeWords()
corpusShark =tm_map(corpusShark, removeWords, c(stopwords("english")))
#Stem Document
corpusShark = tm_map(corpusShark,stemDocument)
#Creating Document Term Matrix(DTM)
frequenciesShark = DocumentTermMatrix(corpusShark)
frequenciesShark
#nspecting the corpus data Looking at matrix
inspect(frequenciesShark[1000:1005,505:515])
#Check for sparsity
findFreqTerms(frequenciesShark,lowfreq = 20)
#Remove Sparse terms
sparseShark = removeSparseTerms(frequenciesShark, 0.995)
#Convert to a data frame
SharkSparse = as.data.frame(as.matrix(sparseShark))
#Make all variable names R-friendly
colnames(SharkSparse) = make.names(colnames(SharkSparse))
#Add depdendent variable
SharkSparse$DV = shark$deal
SharkSparse$DV = as.factor(SharkSparse$DV)
#2 Building Model
sharkModelLR=glm(DV~.,data=SharkSparse,family="binomial")
summary(sparkModelLR)
#3 Finding accuracy
sharkLRPred=predict(sharkModelLR,data=SharkSparse,type="response")
CM=table(SharkSparse$deal,sharkLRPred>0.5)
CM[3]
CM=table(SharkSparse$DV,sharkLRPred>0.5)
CM[3]
Accuracy=(CM[1]+CM[4])/4601
Accuracy
#Building Rpart model
library(rpart)
library(rpart.plot)
sharkModelCART=rpart(DV~.,data=SharkSparse,method="class")
#Interpreting resultss
prp(sharkModelCART, extra=2)
# 1.1
trials = read.csv("\Users\balajiv\Downloads\ct.csv", stringsAsFactors=FALSE)
# 1.1
trials = read.csv("/Users/balajiv/Downloads/ct.csv", stringsAsFactors=FALSE)
# 1.1
trials = read.csv("/Users/balajivr/Downloads/ct.csv", stringsAsFactors=FALSE)
library(tm)
library(SnowballC)
# 1.1
trials = read.csv("/Users/balajivr/Downloads/ct.csv", stringsAsFactors=FALSE)
library(tm)
library(SnowballC)
View(trials)
corpusTitle = Corpus(VectorSource(trials$title))
corpusAbstract = Corpus(VectorSource(trials$abstract))
corpusTitle = tm_map(corpusTitle, tolower)
corpusAbstract = tm_map(corpusAbstract, tolower)
corpusTitle = tm_map(corpusTitle, removePunctuation)
corpusAbstract = tm_map(corpusAbstract, removePunctuation)
corpusTitle = tm_map(corpusTitle, removeWords, stopwords("english"))
corpusAbstract = tm_map(corpusAbstract, removeWords, stopwords("english"))
corpusTitle = tm_map(corpusTitle, stemDocument)
corpusAbstract = tm_map(corpusAbstract, stemDocument)
dtmTitle = DocumentTermMatrix(corpusTitle)
dtmAbstract = DocumentTermMatrix(corpusAbstract)
dtmTitle = removeSparseTerms(dtmTitle, 0.95)
dtmAbstract = removeSparseTerms(dtmAbstract, 0.95)
dtmTitle = as.data.frame(as.matrix(dtmTitle))
dtmAbstract = as.data.frame(as.matrix(dtmAbstract))
ncol(dtmTitle)
ncol(dtmAbstract)
# 3.1
colnames(dtmTitle) = paste0("T", colnames(dtmTitle))
colnames(dtmAbstract) = paste0("A", colnames(dtmAbstract))
# 3.2
dtm = cbind(dtmTitle, dtmAbstract)
dtm$trials = trials$trial
# 3.4
library(rpart)
library(rpart.plot)
trialCART = rpart(trials~., data=dtm, method = "class")
prp(trialCART, extra=2)
View(trials)
library(twitteR)
library(RCurl)
library(httr)
library(tm)
library(wordcloud)
library(syuzhet)
consumer_key = "5OaZGOUPe09zxwfXV8CvtKTIR"
consumer_secret = "uG5i6pwG5cq3r9QOUCImfovySz8R95VQqsWB6DbLHrohrfhu5R"
access_token = "20418344-9ZjdHTkvL6twtHAlw5O6rdfEOnPU4S2RV1EuUIcF9"
access_secret ="3Ve6e2p9r1NKK4USVRvU5YAPGiidNbaFBTwmnoPBIXdg3"
setup_twitter_oauth(consumer_key,consumer_secret,access_token, access_secret)
tweets = searchTwitter("mannkibhat", n = 200, lang = "en")
tweets.df = twListToDF(tweets)
write.csv(tweets.df,file="/Users/balajiv/Downloads/moditweets.csv")
tweets = searchTwitter("narendramodi", n = 200, lang = "en")
tweets.df = twListToDF(tweets)
write.csv(tweets.df,file="/Users/balajiv/Downloads/moditweets.csv")
write.csv(tweets.df,file="/Users/balajivr/Downloads/moditweets.csv")
gaQueries =read.csv("/Users/balajivr/Downloads/GA_Queries 20181124-20181130.csv",stringsAsFactors =FALSE)
attach(gaQueries )
View(gaQueries)
gaQueries =read.csv("/Users/balajivr/Downloads/GAQueries.csv",stringsAsFactors =FALSE)
gaQueries =read.csv("/Users/balajivr/Downloads/GAQueries.csv",stringsAsFactors =FALSE)
attach(gaQueries )
View(gaQueries)
View(gaQueries)
#create corpus
googleCorpus = Corpus(VectorSource(gaQueries$Search.Query))
googleCorpus = tm_map(googleCorpus,removeWords,stopwords("english"))
googleCorpus = tm_map(googleCorpus,stemDocument)
dtmAdded = DocumentTermMatrix(googleCorpus)
googleDTM = DocumentTermMatrix(googleCorpus)
length(stopwords("english"))
googleDTM
googleSparse = removeSparseTerms(googleDTM, .995)
googleSparse
google2 = gaQueries
googleDF = as.data.frame(as.matrix(googleSparse))
google2 = read.csv("/Users/balajivr/Downloads/GAQueries.csv")
googleDF$ap=as.numeric(google2$Average.Position)
googleDF$Impression = NULL
googleLR = lm(ap~.,data=googleDF)
summary(googleLR)
knitr::opts_chunk$set(echo = TRUE)
# transpose the matrix to cluster documents (tweets)
m2 <- t(m)
knitr::opts_chunk$set(echo = TRUE)
print ("Initialize all required library to arrive at the Solution")
library(twitteR)
library(SnowballC)
library(tm)
library(syuzhet)
library(RCurl)
library(httr)
library(wordcloud)
print(" Setting Twitter Account Key, Secret and access token")
consumer_key <- "LnOq4Kro0pSmkA4QNyoZJ2flC"
consumer_secret <- "7IoZcvGlGevlSBPuyWR1X5HocKJD8fTD5KKOQOUTGNoTIwVJ4y"
access_token <- "39058787-PIm4VaFPAwdTLLB0wFcIbWez64MWY4NKTXqPB7jY3"
access_secret <- "9Uqc6CHZWLtcNlY1KTJAT8FHIRa8b5GEP4o8gA3RKTDFv"
setup_twitter_oauth(consumer_key,consumer_secret,access_token, access_secret)
#Fetching tweets from twitter handle @Tesla
tweets <- userTimeline("Tesla", n=1000)
#Total 191 tweets from its timeline
n.tweets <- length(tweets)
#Cleaning tweets for further analysis
tweets.df <- twListToDF(tweets)
#16 variables using userTimeline function
head(tweets.df)
#The field text contains the tweet part along with hashtags and URLs.
#The hashtags and URLs need to be removed to carry out sentiment analysis
head(tweets.df$text)
#URLs, hashtags and other twitter handles are removed using gsub
tweets.df2 <- gsub("http.*","",tweets.df$text)
tweets.df2 <- gsub("http.*","",tweets.df2)
tweets.df2 <- gsub("#.*","",tweets.df2)
tweets.df2 <- gsub("@.*","",tweets.df2)
#Output
head(tweets.df2)
word.df <- as.vector(tweets.df2)
emotion.df <- get_nrc_sentiment(word.df)
emotion.df2 <- cbind(tweets.df2,emotion.df)
#The output shows different emotions present in each of the tweets
head(emotion.df2)
#Get the sentiment score for each of the tweets
sent.value <- get_sentiment(word.df)
most.positive <- word.df[sent.value == max(sent.value)]
most.negative <- word.df[sent.value <= min(sent.value)]
#Gives the most positive and most negative tweet
#Classifying all tweets as positive, negative and neutral
tweets_senti <- ifelse(sent.value < 0, "Negative", ifelse(sent.value > 0, "Positive", "Neutral"))
table(tweets_senti)
tweets_users <- searchTwitter("Tesla", n = 2000, lang = "en")
#Convert list to dataframe
tweets_users.df <- twListToDF(tweets_users)
#URLs, hashtags, retweets and other twitter handles are removed using gsub
tweets_users.df2 = gsub("&amp", "", tweets_users.df$text)
tweets_users.df2 = gsub("(RT|via)((?:\\b\\W*@\\w+)+)", "", tweets_users.df2)
tweets_users.df2 = gsub("@\\w+", "", tweets_users.df2)
tweets_users.df2 = gsub("[[:punct:]]", "", tweets_users.df2)
tweets_users.df2 = gsub("[[:digit:]]", "", tweets_users.df2)
tweets_users.df2 = gsub("http\\w+", "", tweets_users.df2)
tweets_users.df2 = gsub("[ \t]{2,}", "", tweets_users.df2)
tweets_users.df2 = gsub("^\\s+|\\s+$", "", tweets_users.df2)
tweets_users.df2 <- iconv(tweets_users.df2, "UTF-8", "ASCII", sub="")
sent.value <- get_sentiment(tweets_users.df2)
#Initially neutral, then positive and lastly negative. Investigate further if this negative sentiment is related to some event
simple_plot(sent.value)
nrc_sent.value = get_nrc_sentiment(tweets_users.df2)
nrc_sent.value   #Paste a snapshot of the output
#convert to vector source in order to use tm package for data preprocessing
tweets_users.Corpus <- Corpus(VectorSource(tweets_users.df2))
#Inspect the data. Needs data preprocessing.
inspect(tweets_users.Corpus)  #Paste a snapshot of the output
tweets_preprocess <- function(object){
object <- tm_map(object, stripWhitespace)      #Eliminate extra white space
object <- tm_map(object, content_transformer(tolower))   #convert to lower case
object <- tm_map(object, removeWords, c("tesla", "elon", "musk", stopwords("english")))
return(object)
}
tweets_preprocess.Corpus <- tweets_preprocess(tweets_users.Corpus)
dtm.Tweets <- DocumentTermMatrix(tweets_preprocess.Corpus)
inspect(dtm.Tweets)   #Paste a snapshot of the output
#Find frequent terms
findFreqTerms(dtm.Tweets,5)    #Paste a snapshot of the output
#Association with atleast .8 correlation with the term great
findAssocs(dtm.Tweets,c("luxury","scientific"),0.8)  #Paste a snapshot of the output
#Creating the wordcloud
tdm <- TermDocumentMatrix(tweets_preprocess.Corpus)
m <- as.matrix(tdm)
v <- sort(rowSums(m),decreasing=TRUE)
d <- data.frame(word = names(v),freq=v)
wordcloud(d$word,d$freq,max.words = 100)
# transpose the matrix to cluster documents (tweets)
m2 <- t(m)
# set a fixed random seed
set.seed(100)
# k-means clustering of tweets
k <- 8
kmeansResult <- kmeans(m2, k)
# cluster centers
round(kmeansResult$centers, digits=3)
for (i in 1:k) {
cat(paste("cluster ", i, ": ", sep=""))
s <- sort(kmeansResult$centers[i,], decreasing=T)
cat(names(s)[1:3], "\n")
}
#Build K means cluster model
#K-means clustering
library(fpc)
w <- dist(t(m), method="euclidian")
kfit <- kmeans(w,4)
clusplot(as.matrix(w), kfit$cluster, color=T, shade=T, labels=2, lines=0)
#Build K means cluster model
#K-means clustering
library(fpc)
library(cluster)
w <- dist(t(m), method="euclidian")
kfit <- kmeans(w,4)
clusplot(as.matrix(w), kfit$cluster, color=T, shade=T, labels=2, lines=0)
install.packages("tinytex")
library(Hmisc)
mydata <- sasxport.get("/Users/balajivr/Desktop/BABI/Capstone/Health/LLCP2017.XPT")
setwd("/Users/balajivr/Desktop/BABI/Capstone/Health")
getwd()
library(DataExplorer)
prData <- read.csv("new_cdc_data.csv", header = TRUE)
diabData <- prData[1:18]
diabData$PE_freq<-ifelse((diabData$exeroft1>=100 & diabData$exeroft1 <200),((diabData$exeroft1%%100)/7)*30,(diabData$exeroft1%%200))
View(diabData)
diabData$PE_freq<- round(diabData$PE_freq)
View(diabData)
diabData$SE_freq<-ifelse((diabData$exeroft2>=100 & diabData$exeroft2 <200),((diabData$exeroft2%%100)/7)*30,(diabData$exeroft2%%200))
diabData$SE_freq<- round(diabData$SE_freq)
View(diabData)
diabData$diabete3
